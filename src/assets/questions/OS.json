{
  "subject": "Operating Systems - 50 Interview Questions",
  "total_questions": 50,
  "difficulty": "Fresher to Intermediate",
  "answer_length": "50-70 words",
  "companies": ["Infosys", "TCS", "Wipro", "Accenture", "HCL", "Cognizant", "Capgemini", "Deloitte", "Amazon", "Microsoft", "Google", "Adobe", "Flipkart", "Oracle", "IBM"],
  "questions": [
    {"id": 1, "question": "What is an Operating System?", "answer": "An OS is system software managing hardware resources and providing services to user programs. It controls CPU, memory, I/O devices, and files. Acts as intermediary between applications and hardware. Manages process scheduling, memory allocation, security, and provides user interface. Examples: Linux, Windows, macOS. Runs in privileged kernel mode with direct hardware access while restricting user applications to user mode."},
    {"id": 2, "question": "What are the main functions of OS?", "answer": "Main functions: 1) Memory management - allocates/deallocates RAM; 2) Process management - creates, schedules processes; 3) Device management - controls I/O devices; 4) File management - stores/retrieves files; 5) Security - authentication, authorization; 6) Job accounting - tracks performance; 7) Error handling - manages exceptions. These functions ensure efficient resource utilization, system stability, security, and fair resource distribution among competing processes."},
    {"id": 3, "question": "Difference between Process and Thread?", "answer": "Process: Independent program with own memory space, resources, PID. Protected from other processes. Expensive to create (fork). Slow context switching. Threads: Lightweight execution units within process sharing memory. Cheaper to create. Fast context switches. Multiple threads in process can crash entire process. Threads enable parallelism within single process. Processes provide isolation and protection."},
    {"id": 4, "question": "What is Context Switching?", "answer": "Context switching saves running process state (PC, registers, stack pointer) to PCB, loads next process context. Steps: 1) Save current context; 2) Update PCB; 3) Select next process; 4) Load new context; 5) Resume execution. Overhead includes direct cost (save/load time) and indirect cost (TLB flushes, cache misses). Excessive switching reduces performance (thrashing). Context switch time typically microseconds to milliseconds."},
    {"id": 5, "question": "What is Deadlock?", "answer": "Deadlock: Processes blocked indefinitely waiting for resources held by other processes creating circular wait. Coffman's four conditions: mutual exclusion, hold-and-wait, no preemption, circular wait. All must exist simultaneously. Prevention: break any condition. Detection: wait-for graph detects cycles. Recovery: kill processes or rollback transactions. Common in database locking, file access, banker's algorithm prevents allocation."},
    {"id": 6, "question": "What is Virtual Memory?", "answer": "Virtual memory creates illusion of larger memory than physically available using disk. Divides memory into pages (paging) or segments (segmentation). Page faults load missing pages from disk. Advantages: programs larger than RAM, memory isolation, efficient sharing. Disadvantages: page fault overhead (disk access slow), thrashing if excessive paging. Uses TLB for fast address translation."},
    {"id": 7, "question": "Paging vs Segmentation?", "answer": "Paging: fixed-size pages (4KB), transparent to users, no external fragmentation, simple implementation. Segmentation: variable-size segments, programmer-visible, no internal fragmentation, external fragmentation occurs, complex relocation. Paging: efficient memory utilization, predictable. Segmentation: better protection per segment. Modern systems use paging. Some use segmented paging combining both."},
    {"id": 8, "question": "What is Demand Paging?", "answer": "Demand paging loads pages on-demand (lazy loading) instead of loading entire process. Page fault occurs when accessing page not in memory. OS retrieves page from disk, loads into RAM, updates page table, retries instruction. Benefits: saves memory, fast startup. Disadvantages: page fault overhead (20-50ms disk access), thrashing if excessive faults. Works via virtual memory mechanism."},
    {"id": 9, "question": "What is Thrashing?", "answer": "Thrashing: excessive paging causing system to spend more time swapping pages than executing. Causes: insufficient RAM, too many processes, poor page replacement. Symptoms: CPU idle despite disk activity, response time increases, throughput drops. Prevention: reduce multiprogramming level, increase RAM, better page replacement algorithms (LRU), load control. Working set model predicts memory needs."},
    {"id": 10, "question": "What are System Calls?", "answer": "System calls: interface mechanisms for user programs to request kernel services. Cause mode switch from user to kernel mode. Categories: process (fork, exec, exit), file (open, read, write, close), memory (malloc, brk), I/O (ioctl, select), communication (socket, pipe), signals, time. Examples: read(fd, buffer, count), write(fd, buffer, count). Overhead significant - batching reduces syscall frequency improves performance."},
    {"id": 11, "question": "What is Semaphore?", "answer": "Semaphore: synchronization primitive using counter. wait() decrements if positive, otherwise blocks. signal() increments and wakes waiting process. Types: binary (0/1, like mutex), counting (multiple copies). Used for mutual exclusion, producer-consumer problems, resource pools. Atomicity required - hardware or software ensures wait/signal indivisible. Prevents race conditions when properly used."},
    {"id": 12, "question": "What is Mutex vs Semaphore?", "answer": "Mutex: binary lock (locked/unlocked), owned (only owner unlocks), simpler, faster, prevents priority inversion. Semaphore: counting (manages multiple resources), no ownership (anyone can signal), more flexible. Mutex: simple mutual exclusion. Semaphore: complex synchronization patterns. Both ensure only one accesses shared resource. Mutex cleaner for simple cases, semaphore for resource pools."},
    {"id": 13, "question": "Preemptive vs Non-preemptive Scheduling?", "answer": "Preemptive: OS can forcibly remove process from CPU (timer interrupt, higher priority arrives). Examples: Round Robin, Priority scheduling. Better responsiveness, prevents starvation, more overhead. Non-preemptive: process runs until completion/blocking. Examples: FCFS, SJF. Simple, less overhead, high average wait time, starvation possible. Modern systems use preemptive for responsiveness and fairness."},
    {"id": 14, "question": "Explain FCFS Scheduling", "answer": "FCFS (First Come First Served): processes execute in arrival order. Non-preemptive. Simple implementation. Problems: convoy effect (short job waits for long job), high average wait time, poor responsiveness. Process A(5ms), B(10ms), C(15ms) wait times: 0, 5, 15 average=6.67ms. Suitable for batch systems, unsuitable for interactive systems."},
    {"id": 15, "question": "Explain SJF Scheduling", "answer": "SJF (Shortest Job First): process with shortest burst time executes first. Minimizes average wait time (provably optimal). Non-preemptive or preemptive (SRTF). Problems: starvation of long jobs, requires knowing burst time beforehand (impractical). Example: A(5), B(10), C(15) order: A→B→C wait times: 0, 5, 15 average=6.67ms. Better than FCFS."},
    {"id": 16, "question": "Explain Round Robin Scheduling", "answer": "Round Robin: each process gets fixed time quantum (10-100ms). Preemptive. After quantum expires, process goes to queue end. Fair CPU allocation, good responsiveness, prevents starvation. Disadvantage: context switch overhead, performance depends on quantum size (too small=overhead, too large≈FCFS). Suitable for time-sharing systems. Typically average wait time higher than SJF."},
    {"id": 17, "question": "What is Priority Scheduling?", "answer": "Priority scheduling: highest priority process executes first. Preemptive or non-preemptive. Problem: starvation of low-priority processes (continuous high-priority arrivals starve low). Solution: aging - gradually increase waiting process priority. Preemptive: high-priority interrupts low-priority. Used in real-time systems. Important processes get CPU quickly. Requires careful priority assignment."},
    {"id": 18, "question": "What is Memory Fragmentation?", "answer": "Memory fragmentation: memory becomes scattered, unusable. Types: internal (allocated space exceeds need, last page wastes space), external (free memory in small blocks preventing allocation despite total free>needed). Paging eliminates external. Solutions: compaction (move processes together - expensive), better allocation algorithms, virtual memory. Reduces system efficiency and available memory."},
    {"id": 19, "question": "What is Race Condition?", "answer": "Race condition: multiple processes access shared data simultaneously, result depends on execution order. Causes: concurrent access without synchronization. Example: two processes incrementing counter simultaneously lose updates. Problem: unpredictable results, data corruption. Solutions: mutual exclusion (locks, semaphores), monitors, message passing, atomic operations. Prevention: critical sections, proper synchronization."},
    {"id": 20, "question": "What is Critical Section?", "answer": "Critical section: code segment accessing shared resources. Only one process should execute at any time preventing race conditions. Requirements: 1) Mutual exclusion - only one in section; 2) Progress - if none in section, any can enter; 3) Bounded waiting - process shouldn't wait indefinitely. Entry/exit sections protect critical section. Implemented via locks, semaphores, monitors."},
    {"id": 21, "question": "What is Banker's Algorithm?", "answer": "Banker's algorithm: deadlock avoidance checking if resource allocation is safe before granting. Ensures system never enters unsafe state. Processes declare maximum resource needs. Algorithm: tentatively allocate, check if system remains safe (all processes can complete), if safe allocate else delay. Conservative approach. Disadvantages: requires knowing maximum needs, reduces efficiency, complex."},
    {"id": 22, "question": "What is User Mode vs Kernel Mode?", "answer": "User mode: applications run here, restricted direct hardware access, system calls needed for kernel services, failure doesn't crash OS, separate virtual address space per process. Kernel mode: OS runs here, direct resource access, privileged instructions, failure crashes OS, shared kernel memory. Mode switch via interrupts/system calls. User mode provides security, stability. Kernel mode provides performance."},
    {"id": 23, "question": "What is Interrupt?", "answer": "Interrupt: signal halting CPU execution for urgent task. Types: hardware (I/O devices), software (programs), exceptions (CPU errors). Handling: CPU saves context, switches to kernel mode, invokes ISR (interrupt service routine), processes interrupt, restores context, resumes. Enables responsive system, efficient I/O. Interrupt priority determines handling order. Essential for managing asynchronous events."},
    {"id": 24, "question": "What is Starvation in Scheduling?", "answer": "Starvation: process waiting indefinitely for resource/CPU. Occurs with priority scheduling when high-priority processes continuously get CPU. Example: low-priority never scheduled if high-priority always available. Solutions: aging (increase priority with wait time), round robin, fair scheduling. Different from deadlock (deadlock: circular wait; starvation: waiting indefinitely but could run)"},
    {"id": 25, "question": "What is Busy Waiting?", "answer": "Busy waiting: process repeatedly checking condition without sleeping. Loop continuously checks without yielding CPU. Disadvantages: wastes CPU cycles, reduces efficiency, generates unnecessary load. Example: while(lock_free); Inefficient. Solutions: sleep instead of spinning, semaphores, condition variables, event-driven programming. Should be avoided for system efficiency."},
    {"id": 26, "question": "What is Priority Inversion?", "answer": "Priority inversion: low-priority task indirectly blocks high-priority. Low-priority holds resource needed by high-priority. Example: low-priority holds lock, high-priority waits. Defeats priority scheduling. Consequences: unpredictable delays, real-time violations. Solutions: priority inheritance (low task inherits high priority), priority ceiling protocol, resource design. Important in real-time systems."},
    {"id": 27, "question": "What is Aging in Scheduling?", "answer": "Aging: gradually increases waiting process priority preventing starvation. Each time quantum increases priority. Prevents indefinite postponement. Implementation: maintain age counter, increment each cycle, use age in priority calculation. Eventually all processes get high priority ensuring execution. Fairness mechanism in priority scheduling. Balances responsiveness with fairness."},
    {"id": 28, "question": "What is Spooling?", "answer": "Spooling (Simultaneous Peripheral Operation On-Line): temporary storage for I/O devices. Decouples I/O device speed from CPU. Process: CPU writes to disk buffer instead of slow printer, spooler sends to printer later, CPU continues. Benefits: faster job turnaround, multiple jobs queued, better CPU utilization, printer sharing. Used for printers, tape drives. Enables efficient I/O."},
    {"id": 29, "question": "What is Page Replacement?", "answer": "Page replacement: selecting victim page when main memory full. Process: 1) page fault occurs; 2) select victim page; 3) swap with disk; 4) load new page. Algorithms: FIFO (oldest), LRU (least recently used), optimal (used furthest future), clock (circular). Goal: minimize page faults. LRU typically efficient. Trade-off between implementation complexity and efficiency."},
    {"id": 30, "question": "What is Belady's Anomaly?", "answer": "Belady's anomaly: phenomenon where increasing page frames increases page faults (counterintuitive). Occurs with FIFO algorithm. Doesn't occur with optimal algorithm or LRU. Significance: shows FIFO problematic, motivates better algorithms. Solutions: use non-vulnerable algorithms (LRU), implement optimal algorithm theoretically. Demonstrates algorithm choice critically important for performance."},
    {"id": 31, "question": "What is Working Set?", "answer": "Working set: pages process actively using in time window. Dynamic, changes over time. Importance: prevents thrashing (keeping working set in memory prevents faults), predicts fault rate, memory allocation optimization. If allocated memory >= working set size, minimal faults. Larger working set needs more memory. Model used for memory management decisions."},
    {"id": 32, "question": "What is TLB (Translation Lookaside Buffer)?", "answer": "TLB: cache storing recent virtual-to-physical address translations. Located in MMU. Benefits: speeds address translation, reduces memory access overhead, improves performance. Operation: check TLB first (hit uses translation), miss consults page table, updates TLB. TLB hit rate critical (95-99% typical). Faster than page table lookup. Essential for performance in paged systems."},
    {"id": 33, "question": "What is Copy-on-Write (CoW)?", "answer": "Copy-on-Write: optimization postponing data copying until modification. Process: 1) parent-child share pages initially; 2) marked read-only; 3) write attempts cause fault; 4) page copied. Benefits: reduces memory usage, faster fork(), improves performance. Used in process creation, virtual memory. Reduces unnecessary copying overhead. Efficient process spawning. Transparent to applications."},
    {"id": 34, "question": "What is Mutual Exclusion?", "answer": "Mutual exclusion: only one process accesses shared resource at time. Fundamental synchronization concept. Methods: locks, semaphores, monitors, message passing. Requirements: safety (prevents race), liveness (prevents deadlock), fairness (equitable access). Implementation considerations: overhead, complexity, performance impact. Essential for concurrent systems. Enables safe resource sharing."},
    {"id": 35, "question": "What is Monitor?", "answer": "Monitor: high-level synchronization combining mutex and condition variables. Encapsulates shared data. Features: automatic mutual exclusion, condition variables for waiting. Operations: only one process in monitor, waiting queue for blocked processes. Advantages: prevents deadlock easily, higher abstraction, easier programming. Used in concurrent languages (Java). More structured than semaphores."},
    {"id": 36, "question": "What is Condition Variable?", "answer": "Condition variable: synchronization primitive allowing processes to wait for conditions. Operations: wait() (releases mutex, waits), signal() (wakes one), broadcast() (wakes all). Used with mutex for efficient waiting. Example: producer-consumer. Process acquires mutex, checks condition, if false waits, when signaled retries. Efficient vs busy waiting. Standard concurrency mechanism."},
    {"id": 37, "question": "What is Message Passing IPC?", "answer": "Message passing: inter-process communication via messages. Processes send/receive messages. Models: direct (named process), indirect (mailbox). Synchronization: blocking (waits), non-blocking (returns). Advantages: no shared memory, naturally distributed, easy synchronization. Disadvantages: overhead, slower than shared memory. Used in distributed systems, microservices. Clean isolation between processes."},
    {"id": 38, "question": "What is Shared Memory IPC?", "answer": "Shared memory: fastest IPC mechanism, common memory region accessed by processes. Process: create segment, attach to address space, read/write, synchronize access. Advantages: fast (no kernel overhead per message), efficient large data. Disadvantages: requires synchronization, race conditions possible, complex debugging. Used when performance critical. Needs careful synchronization."},
    {"id": 39, "question": "What is File Descriptor?", "answer": "File descriptor: non-negative integer referencing open file. Process maintains table. Standard: 0 (stdin), 1 (stdout), 2 (stderr). Operations: open() allocates, read/write uses, close() releases. Refers to file state: position, flags, permissions. Each process independent descriptors. Limited per process. Essential for file operations. Provides abstraction over file representation."},
    {"id": 40, "question": "What is Inode in File System?", "answer": "Inode (index node): data structure storing file metadata. Contains: size, owner, permissions, timestamps, disk block pointers, type. Each file has inode. Inode number identifies file. Operations reference inode. Hard link shares inode. Symlink different inode pointing to name. Inode table stores all. Essential file system structure."},
    {"id": 41, "question": "What is Symbolic Link?", "answer": "Symbolic link (symlink): file containing path to another file. Acts as shortcut. Types: absolute path (full), relative (relative to symlink). Properties: different inode, points to name (not inode), can cross filesystems, can be broken, can create cycles. Creation: ln -s target link. Advantages: flexible, safe to delete target. Different from hard link."},
    {"id": 42, "question": "What is Hard Link?", "answer": "Hard link: directory entry pointing to inode. Multiple names for same file. Properties: same inode, same permissions/content, cannot cross filesystems, deleting one doesn't affect others. Link count tracked. Creation: ln target link. Benefits: efficient sharing, transparent. Limitations: cannot link directories, cannot cross filesystems. Different from symbolic link."},
    {"id": 43, "question": "What is Caching in OS?", "answer": "Caching: storing frequently accessed data in faster memory. Levels: CPU cache (L1/L2/L3), TLB, page cache, buffer cache. Benefits: faster access, reduced latency, improved performance. Policies: replacement algorithm (LRU, FIFO), write policy (write-through, write-back). Hit rate critical. Misses costly. Transparent to programs. Essential for performance."},
    {"id": 44, "question": "What is Disk Scheduling?", "answer": "Disk scheduling: ordering I/O requests minimizing seek time. Algorithms: FCFS (simple, inefficient), SSTF (shortest seek), SCAN (elevator), C-SCAN (circular), LOOK (modified SCAN). Goal: minimize seek time, improve throughput, reduce latency. Seek time dominates. SCAN/C-SCAN good for large workloads. Algorithm choice impacts performance significantly."},
    {"id": 45, "question": "What is Socket?", "answer": "Socket: endpoint for network communication. Combines IP address, protocol, port. Types: stream (TCP, reliable), datagram (UDP, unreliable), raw (direct IP). Operations: socket(), bind(), listen(), accept(), connect(), send(), recv(), close(). Used for client-server communication, network programming. Abstraction hiding complexity. Essential for network applications."},
    {"id": 46, "question": "What is Fork?", "answer": "Fork: system call creating new process (child) from existing (parent). Child nearly identical to parent. Properties: returns child PID to parent, 0 to child, copies memory, inherits file descriptors, separate process space. Benefits: process creation, multitasking. Post-fork typically exec() called. Foundation of UNIX process model. Enables process management."},
    {"id": 47, "question": "What is Exec?", "answer": "Exec: system call replacing process image with new program. Transforms current process. Variants: execl(), execv(), execle(), execve(). Properties: same PID, same file descriptors, same limits, new code/data. Returns only on error. Usually follows fork(). Typical: fork creates process, exec loads program. Enables program execution."},
    {"id": 48, "question": "What is Zombie Process?", "answer": "Zombie process: terminated but parent hasn't reaped exit status. Remains in process table. Causes: parent doesn't call wait(). Properties: consumes PID, minimal resources, shows <defunct> in ps. Prevention: parent calls wait(), signal handlers, init adoption. Harmless individually but indicates bug. Solution: parent must reap. Shows in process table."},
    {"id": 49, "question": "What is Orphan Process?", "answer": "Orphan process: parent terminated. Automatically adopted by init (PID 1). Causes: parent terminates, parent crash. Properties: new parent init, init reaps status, no resources leaked, visible as init child. Generally harmless. init adoption prevents problems. Different from zombie. Resources properly managed. Acceptable in UNIX."},
    {"id": 50, "question": "What is Signal in OS?", "answer": "Signal: software interrupt notifying process of event. Asynchronous communication. Common: SIGTERM (terminate), SIGKILL (kill), SIGSTOP (stop), SIGSEGV (segfault). Properties: handled by handlers, can block, reliable delivery. Operations: signal(), sigaction(), kill(). Usage: termination, timeout, errors, I/O completion. Essential process control mechanism."}
  ]
}